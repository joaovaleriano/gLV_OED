{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import combinations\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from analysis import *\n",
    "from inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_species = [ 3  5 10 20]\n",
      "avg_samp_dt = [3.  1.5 1.  0.6 0.3]\n",
      "env_noise = 0.1\n",
      "meas_noise_list = [0.1]\n",
      "n_params_seeds = 10\n"
     ]
    }
   ],
   "source": [
    "datapath = \"../experiment_outputs/growth_scale_0.1_env_noise0.1\"\n",
    "log = h5py.File(f\"{datapath}/data_generation_log.h5\", \"r\")\n",
    "\n",
    "print(f\"n_species = {log.attrs['n_species']}\")\n",
    "print(f\"avg_samp_dt = {log.attrs['avg_samp_dt']}\")\n",
    "print(f\"env_noise = {log.attrs['env_noise']}\")\n",
    "print(f\"meas_noise_list = {log.attrs['meas_noise_list']}\")\n",
    "print(f\"n_params_seeds = {log.attrs['n_params_seeds']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt, filetype=\"dataset\", ext=\"csv\"):\n",
    "    params_seeds = [i.split(\"param_seed\")[1] for i in os.listdir(f\"{datapath}/{n_sp}_sp\")]\n",
    "\n",
    "    datafiles = []\n",
    "\n",
    "    for p in params_seeds:\n",
    "        datafiles.append(f\"{datapath}/{n_sp}_sp/param_seed{p}/meas_noise{meas_noise}/t_samp{avg_samp_dt}/{filetype}{n_sp}_sp{p}_env_noise{env_noise}.{ext}\")\n",
    "    return datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of sampling points: [ 11  21  31  51 101]\n",
      "Average sampling intervals: [3.  1.5 1.  0.6 0.3]\n",
      "Number of initial conditions: 30\n",
      "Number of repetitions: 1\n",
      "Environmental noise: 0.1\n",
      "Amounts of measurement noise: [0.1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numbers of sampling points: {log.attrs['n_samples']}\")\n",
    "print(f\"Average sampling intervals: {log.attrs['avg_samp_dt'].round(3)}\")\n",
    "print(f\"Number of initial conditions: {log.attrs['n_init_cond']}\")\n",
    "print(f\"Number of repetitions: {log.attrs['repetitions']}\")\n",
    "print(f\"Environmental noise: {log.attrs['env_noise']}\")\n",
    "print(f\"Amounts of measurement noise: {log.attrs['meas_noise_list']}\")\n",
    "\n",
    "env_noise = log.attrs['env_noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_es_score(true_aij, inferred_aij) -> float:\n",
    "    \"\"\"GRANT'S edited version to calculate ED score\n",
    "\n",
    "    Calculate the ecological direction (EDₙ) score (n := number of species in ecosystem).\n",
    "\n",
    "    Parameters\n",
    "    ===============\n",
    "    truth: ndarray(axis0=species_names, axis1=species_names), the ecosystem coefficient matrix used to generate data\n",
    "    inferred: ndarray(axis0=species_names, axis1=species_names), the inferred ecosystem coefficient matrix\n",
    "    Returns\n",
    "    ===============\n",
    "    ES_score: float\n",
    "    \"\"\"\n",
    "\n",
    "    truth = pd.DataFrame(true_aij).copy()\n",
    "    inferred = pd.DataFrame(inferred_aij).copy()\n",
    "\n",
    "    # consider inferred coefficients\n",
    "    mask = inferred != 0\n",
    "\n",
    "    # compare sign: agreement when == -2 or +2, disagreement when 0\n",
    "    nonzero_sign = np.sign(inferred)[mask] + np.sign(truth)[mask]\n",
    "    corr_sign = (np.abs(nonzero_sign) == 2).sum().sum()\n",
    "    opposite_sign = (np.abs(nonzero_sign) == 0).sum().sum()\n",
    "\n",
    "    # count incorrect non-zero coefficients\n",
    "    wrong_nz = (truth[mask] == 0).sum().sum()\n",
    "\n",
    "    # combine\n",
    "    unscaled_score = corr_sign - opposite_sign\n",
    "\n",
    "    # scale by theoretical extrema\n",
    "    truth_nz_counts = (truth != 0).sum().sum()\n",
    "    truth_z_counts = len(truth.index) ** 2 - truth_nz_counts\n",
    "    theoretical_min = -truth_nz_counts\n",
    "    theoretical_max = truth_nz_counts\n",
    "\n",
    "    ES_score = (unscaled_score - theoretical_min) / (theoretical_max - theoretical_min)\n",
    "\n",
    "    return ES_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer and score\n",
    "\n",
    "for n_sp in log.attrs[\"n_species\"]:\n",
    "    for avg_samp_dt in log.attrs[\"avg_samp_dt\"]:\n",
    "        for meas_noise in log.attrs[\"meas_noise_list\"]:\n",
    "            datafiles = get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt)\n",
    "            metadatafiles = get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt, \"metadata\", \"txt\")\n",
    "\n",
    "            for file_idx in range(len(datafiles)):\n",
    "                datafile = datafiles[file_idx]\n",
    "                metadatafile = metadatafiles[file_idx]\n",
    "                metadict = get_meta(open(metadatafile, \"r\").read().split(\"\\n\"))\n",
    "                \n",
    "                df = pd.read_csv(datafile, index_col=0)\n",
    "                \n",
    "                param_columns = [f\"r{i}\" for i in range(1, n_sp+1)] + \\\n",
    "                [f\"A{i},{j}\" for i in range(1, n_sp+1) for j in range(1, n_sp+1)]\n",
    "                cols = [\"n_init_cond\"] + list(df.columns[1:4]) + param_columns + [\"MSPD\", \"CSR\", \"ES\"]\n",
    "\n",
    "                infer_out = pd.DataFrame(columns=cols)\n",
    "\n",
    "                pd.options.mode.chained_assignment = None\n",
    "                \n",
    "                p = metadict[\"parameters\"]\n",
    "                r = p[:n_sp]\n",
    "                A = p[n_sp:].reshape((n_sp,n_sp))\n",
    "\n",
    "                for i in tqdm(range(len(df.init_cond_idx.unique()))):\n",
    "                    combs = list(combinations(df.init_cond_idx.unique(), i+1))\n",
    "                    np.random.shuffle(combs)\n",
    "                    for comb in combs[:100]:\n",
    "                        df_comb = df[df.init_cond_idx.isin(comb)]\n",
    "                        r_est, A_est = fit_ridge_cv(df_comb)\n",
    "                        p_est = np.concatenate((r_est, A_est.flatten()))\n",
    "                        MSPD = ((p-p_est)**2).mean()\n",
    "                        CSR = (np.sign(A_est)==np.sign(A)).mean()\n",
    "                        ES = calculate_es_score(A, A_est)\n",
    "                        infer_out.loc[len(infer_out)] = [i+1, comb, avg_samp_dt, meas_noise] + list(p_est) + [MSPD, CSR, ES]\n",
    "\n",
    "                infer_out.to_csv(datafile.split('dataset')[0]+\"/inference\"+datafile.split(\"dataset\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sp = 10\n",
    "avg_samp_dt = 3.\n",
    "env_noise = 0.1\n",
    "meas_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>init_cond_idx</th>\n",
       "      <th>t_samp_dist_idx</th>\n",
       "      <th>measurement_noise</th>\n",
       "      <th>replicate</th>\n",
       "      <th>time</th>\n",
       "      <th>dt</th>\n",
       "      <th>sp1</th>\n",
       "      <th>sp2</th>\n",
       "      <th>sp3</th>\n",
       "      <th>sp4</th>\n",
       "      <th>sp5</th>\n",
       "      <th>sp6</th>\n",
       "      <th>sp7</th>\n",
       "      <th>sp8</th>\n",
       "      <th>sp9</th>\n",
       "      <th>sp10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022106</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.041375</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.039007</td>\n",
       "      <td>0.105712</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.041970</td>\n",
       "      <td>0.033069</td>\n",
       "      <td>0.126822</td>\n",
       "      <td>0.069387</td>\n",
       "      <td>0.017040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>0.078010</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.049153</td>\n",
       "      <td>0.110442</td>\n",
       "      <td>0.094341</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.044018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.180420</td>\n",
       "      <td>0.068103</td>\n",
       "      <td>0.148239</td>\n",
       "      <td>0.194660</td>\n",
       "      <td>0.046932</td>\n",
       "      <td>0.070541</td>\n",
       "      <td>0.134549</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.156831</td>\n",
       "      <td>0.120309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.253067</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>0.170874</td>\n",
       "      <td>0.211621</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.142457</td>\n",
       "      <td>0.173067</td>\n",
       "      <td>0.042395</td>\n",
       "      <td>0.179181</td>\n",
       "      <td>0.190378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.215978</td>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.197424</td>\n",
       "      <td>0.223782</td>\n",
       "      <td>0.062158</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.239858</td>\n",
       "      <td>0.072690</td>\n",
       "      <td>0.109992</td>\n",
       "      <td>0.248139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.177467</td>\n",
       "      <td>0.075213</td>\n",
       "      <td>0.169093</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.096993</td>\n",
       "      <td>0.137520</td>\n",
       "      <td>0.205123</td>\n",
       "      <td>0.106396</td>\n",
       "      <td>0.138747</td>\n",
       "      <td>0.234307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.175498</td>\n",
       "      <td>0.099151</td>\n",
       "      <td>0.200643</td>\n",
       "      <td>0.353971</td>\n",
       "      <td>0.110002</td>\n",
       "      <td>0.101953</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.097520</td>\n",
       "      <td>0.178762</td>\n",
       "      <td>0.228997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.206518</td>\n",
       "      <td>0.104705</td>\n",
       "      <td>0.195028</td>\n",
       "      <td>0.253211</td>\n",
       "      <td>0.072336</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>0.192780</td>\n",
       "      <td>0.060564</td>\n",
       "      <td>0.166390</td>\n",
       "      <td>0.233049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234502</td>\n",
       "      <td>0.091572</td>\n",
       "      <td>0.177536</td>\n",
       "      <td>0.178328</td>\n",
       "      <td>0.081554</td>\n",
       "      <td>0.116814</td>\n",
       "      <td>0.157422</td>\n",
       "      <td>0.040320</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>0.333625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  init_cond_idx  t_samp_dist_idx  measurement_noise  replicate  \\\n",
       "0        0.0            0.0              0.0                0.1        0.0   \n",
       "1        0.0            0.0              0.0                0.1        0.0   \n",
       "2        0.0            0.0              0.0                0.1        0.0   \n",
       "3        0.0            0.0              0.0                0.1        0.0   \n",
       "4        0.0            0.0              0.0                0.1        0.0   \n",
       "..       ...            ...              ...                ...        ...   \n",
       "325     29.0           29.0              0.0                0.1        0.0   \n",
       "326     29.0           29.0              0.0                0.1        0.0   \n",
       "327     29.0           29.0              0.0                0.1        0.0   \n",
       "328     29.0           29.0              0.0                0.1        0.0   \n",
       "329     29.0           29.0              0.0                0.1        0.0   \n",
       "\n",
       "     time   dt       sp1       sp2       sp3       sp4       sp5       sp6  \\\n",
       "0     0.0  3.0  0.020463  0.029830  0.014310  0.022106  0.007418  0.008575   \n",
       "1     3.0  3.0  0.050150  0.083832  0.039007  0.105712  0.017638  0.041970   \n",
       "2     6.0  3.0  0.118098  0.082531  0.078010  0.152139  0.033296  0.049153   \n",
       "3     9.0  3.0  0.180420  0.068103  0.148239  0.194660  0.046932  0.070541   \n",
       "4    12.0  3.0  0.253067  0.061037  0.170874  0.211621  0.075105  0.142457   \n",
       "..    ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "325  18.0  3.0  0.215978  0.119305  0.197424  0.223782  0.062158  0.122119   \n",
       "326  21.0  3.0  0.177467  0.075213  0.169093  0.253530  0.096993  0.137520   \n",
       "327  24.0  3.0  0.175498  0.099151  0.200643  0.353971  0.110002  0.101953   \n",
       "328  27.0  3.0  0.206518  0.104705  0.195028  0.253211  0.072336  0.102288   \n",
       "329  30.0  NaN  0.234502  0.091572  0.177536  0.178328  0.081554  0.116814   \n",
       "\n",
       "          sp7       sp8       sp9      sp10  \n",
       "0    0.005520  0.041375  0.003918  0.012350  \n",
       "1    0.033069  0.126822  0.069387  0.017040  \n",
       "2    0.110442  0.094341  0.200847  0.044018  \n",
       "3    0.134549  0.053704  0.156831  0.120309  \n",
       "4    0.173067  0.042395  0.179181  0.190378  \n",
       "..        ...       ...       ...       ...  \n",
       "325  0.239858  0.072690  0.109992  0.248139  \n",
       "326  0.205123  0.106396  0.138747  0.234307  \n",
       "327  0.238100  0.097520  0.178762  0.228997  \n",
       "328  0.192780  0.060564  0.166390  0.233049  \n",
       "329  0.157422  0.040320  0.176968  0.333625  \n",
       "\n",
       "[330 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles = get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt)\n",
    "pd.read_csv(datafiles[0], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def n_comb(n, k):\n",
    "    return math.factorial(n)/(math.factorial(n-k)*math.factorial(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = list(combinations(df.dataset.unique(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.,  3., 17.])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_294602/1206716298.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "tuple(comb) in combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:32<00:00,  3.07s/it]\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.86s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.89s/it]\n",
      "100%|██████████| 30/30 [01:23<00:00,  2.77s/it]\n",
      "100%|██████████| 30/30 [01:20<00:00,  2.68s/it]\n",
      "100%|██████████| 30/30 [01:21<00:00,  2.70s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.96s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:33<00:00,  3.12s/it]\n",
      "100%|██████████| 30/30 [01:44<00:00,  3.49s/it]\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.17s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.89s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.08s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.95s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.88s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.95s/it]\n",
      "100%|██████████| 30/30 [01:23<00:00,  2.80s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.80s/it]\n",
      "100%|██████████| 30/30 [01:40<00:00,  3.35s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.00s/it]\n",
      "100%|██████████| 30/30 [01:22<00:00,  2.77s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.99s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.97s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.02s/it]\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.85s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:21<00:00,  2.72s/it]\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.85s/it]\n",
      "100%|██████████| 30/30 [01:44<00:00,  3.49s/it]\n",
      "100%|██████████| 30/30 [01:43<00:00,  3.45s/it]\n",
      "100%|██████████| 30/30 [01:34<00:00,  3.15s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.03s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.95s/it]\n",
      "100%|██████████| 30/30 [01:43<00:00,  3.45s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.98s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.88s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.88s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.94s/it]\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.18s/it]\n",
      "100%|██████████| 30/30 [01:37<00:00,  3.27s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.10s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.01s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.09s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.09s/it]\n",
      "100%|██████████| 30/30 [01:34<00:00,  3.16s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.22s/it]\n",
      "100%|██████████| 30/30 [01:33<00:00,  3.13s/it]\n",
      "100%|██████████| 30/30 [01:33<00:00,  3.12s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.99s/it]\n",
      "100%|██████████| 30/30 [01:33<00:00,  3.11s/it]\n",
      "100%|██████████| 30/30 [01:34<00:00,  3.14s/it]\n",
      "100%|██████████| 30/30 [01:34<00:00,  3.14s/it]\n",
      "100%|██████████| 30/30 [01:34<00:00,  3.14s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.20s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.00s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.94s/it]\n",
      "100%|██████████| 30/30 [01:31<00:00,  3.05s/it]\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.84s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.99s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.10s/it]\n",
      "100%|██████████| 30/30 [01:51<00:00,  3.72s/it]\n",
      "100%|██████████| 30/30 [01:55<00:00,  3.84s/it]\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.84s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.07s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.02s/it]\n",
      "100%|██████████| 30/30 [01:39<00:00,  3.33s/it]\n",
      "100%|██████████| 30/30 [01:33<00:00,  3.13s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.03s/it]\n",
      "100%|██████████| 30/30 [01:14<00:00,  2.47s/it]\n",
      "100%|██████████| 30/30 [01:13<00:00,  2.46s/it]\n",
      "100%|██████████| 30/30 [01:14<00:00,  2.47s/it]\n",
      "100%|██████████| 30/30 [01:14<00:00,  2.48s/it]\n",
      "100%|██████████| 30/30 [01:14<00:00,  2.47s/it]\n",
      "100%|██████████| 30/30 [01:23<00:00,  2.80s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.91s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.81s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.96s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  3.03s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.95s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.96s/it]\n",
      "100%|██████████| 30/30 [01:22<00:00,  2.74s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.56s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.57s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.54s/it]\n",
      "100%|██████████| 30/30 [01:15<00:00,  2.53s/it]\n",
      "100%|██████████| 30/30 [01:15<00:00,  2.53s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.54s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.55s/it]\n",
      "100%|██████████| 30/30 [01:16<00:00,  2.55s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.91s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.88s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.88s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.91s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.87s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.91s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.89s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.89s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.92s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.21s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.22s/it]\n",
      "100%|██████████| 30/30 [01:32<00:00,  3.10s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.18s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.22s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.94s/it]\n",
      "100%|██████████| 30/30 [01:28<00:00,  2.94s/it]\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.20s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.99s/it]\n",
      "100%|██████████| 30/30 [01:37<00:00,  3.26s/it]\n",
      "100%|██████████| 30/30 [01:53<00:00,  3.78s/it]\n",
      "100%|██████████| 30/30 [02:31<00:00,  5.05s/it]\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.20s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.23s/it]\n",
      "100%|██████████| 30/30 [01:36<00:00,  3.22s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.62s/it]\n",
      "100%|██████████| 30/30 [01:49<00:00,  3.66s/it]\n",
      "100%|██████████| 30/30 [02:11<00:00,  4.37s/it]\n",
      "100%|██████████| 30/30 [01:45<00:00,  3.53s/it]\n",
      "100%|██████████| 30/30 [03:42<00:00,  7.43s/it]\n",
      "100%|██████████| 30/30 [03:42<00:00,  7.43s/it]\n",
      "100%|██████████| 30/30 [03:14<00:00,  6.49s/it]\n",
      "100%|██████████| 30/30 [03:21<00:00,  6.71s/it]\n",
      "100%|██████████| 30/30 [03:41<00:00,  7.38s/it]\n",
      "100%|██████████| 30/30 [02:57<00:00,  5.91s/it]\n",
      "100%|██████████| 30/30 [03:37<00:00,  7.25s/it]\n",
      "100%|██████████| 30/30 [03:41<00:00,  7.39s/it]\n",
      "100%|██████████| 30/30 [03:37<00:00,  7.25s/it]\n",
      "100%|██████████| 30/30 [02:40<00:00,  5.36s/it]\n",
      "100%|██████████| 30/30 [03:08<00:00,  6.30s/it]\n",
      "100%|██████████| 30/30 [03:08<00:00,  6.28s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.25s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.24s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.26s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.26s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.25s/it]\n",
      "100%|██████████| 30/30 [03:08<00:00,  6.27s/it]\n",
      "100%|██████████| 30/30 [03:07<00:00,  6.27s/it]\n",
      "100%|██████████| 30/30 [03:08<00:00,  6.28s/it]\n",
      "100%|██████████| 30/30 [04:04<00:00,  8.15s/it]\n",
      "100%|██████████| 30/30 [04:04<00:00,  8.16s/it]\n",
      "100%|██████████| 30/30 [04:05<00:00,  8.19s/it]\n",
      "100%|██████████| 30/30 [04:04<00:00,  8.16s/it]\n",
      "100%|██████████| 30/30 [04:05<00:00,  8.19s/it]\n",
      "100%|██████████| 30/30 [04:04<00:00,  8.15s/it]\n",
      "100%|██████████| 30/30 [04:07<00:00,  8.25s/it]\n",
      "100%|██████████| 30/30 [04:07<00:00,  8.26s/it]\n",
      "100%|██████████| 30/30 [05:36<00:00, 11.20s/it]\n",
      "100%|██████████| 30/30 [05:39<00:00, 11.33s/it]\n",
      "100%|██████████| 30/30 [05:18<00:00, 10.62s/it]\n",
      "100%|██████████| 30/30 [05:13<00:00, 10.45s/it]\n",
      "100%|██████████| 30/30 [05:12<00:00, 10.40s/it]\n",
      "100%|██████████| 30/30 [05:13<00:00, 10.46s/it]\n",
      "100%|██████████| 30/30 [05:12<00:00, 10.43s/it]\n",
      "100%|██████████| 30/30 [05:14<00:00, 10.48s/it]\n",
      "100%|██████████| 30/30 [05:13<00:00, 10.44s/it]\n",
      "100%|██████████| 30/30 [05:21<00:00, 10.71s/it]\n",
      "100%|██████████| 30/30 [05:22<00:00, 10.76s/it]\n",
      "100%|██████████| 30/30 [05:18<00:00, 10.61s/it]\n",
      "100%|██████████| 30/30 [06:15<00:00, 12.52s/it]\n",
      "100%|██████████| 30/30 [06:20<00:00, 12.68s/it]\n",
      "100%|██████████| 30/30 [06:11<00:00, 12.39s/it]\n",
      "100%|██████████| 30/30 [06:10<00:00, 12.35s/it]\n",
      "100%|██████████| 30/30 [06:10<00:00, 12.36s/it]\n",
      "100%|██████████| 30/30 [06:11<00:00, 12.39s/it]\n",
      "100%|██████████| 30/30 [06:10<00:00, 12.34s/it]\n",
      "100%|██████████| 30/30 [06:11<00:00, 12.40s/it]\n",
      "100%|██████████| 30/30 [06:11<00:00, 12.40s/it]\n",
      "100%|██████████| 30/30 [06:10<00:00, 12.34s/it]\n",
      "100%|██████████| 30/30 [06:58<00:00, 13.94s/it]\n",
      "100%|██████████| 30/30 [06:59<00:00, 13.97s/it]\n",
      "100%|██████████| 30/30 [06:58<00:00, 13.96s/it]\n",
      "100%|██████████| 30/30 [06:57<00:00, 13.93s/it]\n",
      "100%|██████████| 30/30 [06:56<00:00, 13.87s/it]\n",
      "100%|██████████| 30/30 [06:56<00:00, 13.90s/it]\n",
      "100%|██████████| 30/30 [06:56<00:00, 13.90s/it]\n",
      "100%|██████████| 30/30 [06:57<00:00, 13.92s/it]\n",
      "100%|██████████| 30/30 [06:58<00:00, 13.95s/it]\n",
      "100%|██████████| 30/30 [06:56<00:00, 13.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Infer and score\n",
    "\n",
    "for n_sp in log.attrs[\"n_species\"]:\n",
    "    for avg_samp_dt in log.attrs[\"avg_samp_dt\"]:\n",
    "        for meas_noise in log.attrs[\"meas_noise_list\"]:\n",
    "            datafiles = get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt)\n",
    "            metadatafiles = get_files(datapath, n_sp, env_noise, meas_noise, avg_samp_dt, \"metadata\", \"txt\")\n",
    "\n",
    "            for file_idx in range(len(datafiles)):\n",
    "                datafile = datafiles[file_idx]\n",
    "                metadatafile = metadatafiles[file_idx]\n",
    "                metadict = get_meta(open(metadatafile, \"r\").read().split(\"\\n\"))\n",
    "                \n",
    "                df = pd.read_csv(datafile, index_col=0)\n",
    "                \n",
    "                param_columns = [f\"r{i}\" for i in range(1, n_sp+1)] + \\\n",
    "                [f\"A{i},{j}\" for i in range(1, n_sp+1) for j in range(1, n_sp+1)]\n",
    "                cols = [\"n_dset\"] + list(df.columns[1:4]) + param_columns + [\"MSPD\", \"CSR\", \"ES\"]\n",
    "\n",
    "                infer_out = pd.DataFrame(columns=cols)\n",
    "\n",
    "                pd.options.mode.chained_assignment = None\n",
    "                \n",
    "                p = metadict[\"parameters\"]\n",
    "                r = p[:n_sp]\n",
    "                A = p[n_sp:].reshape((n_sp,n_sp))\n",
    "\n",
    "                for i in tqdm(range(len(df.dataset.unique()))):\n",
    "                    if n_comb(len(df.dataset.unique()), i+1) < 10000:\n",
    "                        combs = list(combinations(df.dataset.unique(), i+1))\n",
    "                        np.random.shuffle(combs)\n",
    "                        combs = combs[:100]\n",
    "                    else:\n",
    "                        combs = []\n",
    "                        while len(combs) < 100:\n",
    "                            comb = tuple(np.random.choice(df.dataset.unique(), i+1, replace=False))\n",
    "                            if comb not in combs:\n",
    "                                combs.append(comb)\n",
    "                    for comb in combs:\n",
    "                        comb = np.random.choice(df.dataset.unique(), i+1, replace=False)\n",
    "                        df_comb = df[df.dataset.isin(comb)]\n",
    "                        r_est, A_est = fit_ridge_cv(df_comb)\n",
    "                        p_est = np.concatenate((r_est, A_est.flatten()))\n",
    "                        MSPD = ((p-p_est)**2).mean()\n",
    "                        CSR = (np.sign(A_est)==np.sign(A)).mean()\n",
    "                        ES = calculate_es_score(A, A_est)\n",
    "                        infer_out.loc[len(infer_out)] = [i+1, comb, avg_samp_dt, meas_noise] + list(p_est) + [MSPD, CSR, ES]\n",
    "\n",
    "                infer_out.to_csv(datafile.split('dataset')[0]+\"/inference\"+datafile.split(\"dataset\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61f66c9cb52b40e00ac570727ccd8b1767a1d502bed964675cbccc34495792c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
